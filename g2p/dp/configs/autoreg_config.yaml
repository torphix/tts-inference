paths:
  checkpoint_dir: checkpoints # Directory to store model checkpoints and tensorboard, will be created if not existing.
  data_dir: datasets # Directory to store processed data, will be created if not existing.

preprocessing:
  languages: ["en"] # All languages in the dataset.

  char_repeats:
    1 # Number of grapheme character repeats to allow for mapping to longer phoneme sequences.
    # Set to 1 for autoreg_transformer.
  lowercase: true # Whether to lowercase the grapheme input.
  n_val: 7,912 # Default number of validation data points if no explicit validation data is provided.

model:
  type:
    "autoreg_transformer" # Whether to use a forward transformer or autoregressive transformer model.
    # Choices: ['transformer', 'autoreg_transformer']
  d_model: 128
  d_fft: 512
  layers: 4
  dropout: 0.1
  heads: 4

training:
  # Hyperparams for learning rate and scheduler.
  # The scheduler is reducing the lr on plateau of phoneme error rate (tested every n_generate_steps).

  learning_rate: 0.0002 # Learning rate of Adam.
  warmup_steps: 10000 # Linear increase of the lr from zero to the given lr within the given number of steps.
  batch_size: 128 # Training batch size.
  batch_size_val: 128 # Validation batch size.
  generate_steps:
    1000 # Interval of training steps to generate sample outputs. Also, at this step the phoneme and word
    # error rates are calculated for the scheduler.
  validate_steps:
    1000 # Interval of training steps to validate the model
    # (for the autoregressive model this is teacher-forced).
  checkpoint_steps: 10000 # Interval of training steps to save the model.
  n_generate_samples: 10 # Number of result samples to show on tensorboard.
  store_phoneme_dict_in_model:
    true # Whether to store the raw phoneme dict in the model.
    # It will be loaded by the phonemizer object.
  adam_beta1: 0.9
  adam_beta2: 0.998
  scheduler_milestones: [150000, 225000]
  scheduler_gamma: 0.3
  total_steps: 500000
